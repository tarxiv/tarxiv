#!/usr/bin/env python

from confluent_kafka.admin import AdminClient, _TopicPartition, NewTopic
from confluent_kafka import Consumer, KafkaException, Producer
import argparse
import json
import os

# PARSER
parser = argparse.ArgumentParser("kafka-admin")
group = parser.add_mutually_exclusive_group()
group.add_argument("-s", "--submit", type=str, help="JSON file of values to submit")
group.add_argument("-d", "--delete", type=str, help="delete kafka topic")
group.add_argument("-c", "--create", type=str, help="create kafka topic")
group.add_argument("-l", "--list", action="store_true", help="list kafka topics")
group.add_argument("-e", "--explain", type=str, help="explain/describe kafka topic")
parser.add_argument("-p", "--partitions", type=int, default=16,
                    help="number of partitions for new topic")
parser.add_argument("-r", "--replication", type=int, default=1,
                    help="replication factor for new topic")
parser.add_argument("-t", "--topic", type=str, help="kafka topic name (for submission)")
args = parser.parse_args()

# Connect to kafka server
kafka_host = os.environ["TARXIV_KAFKA_HOST"]
admin_client_config = {
    'bootstrap.servers': f"{kafka_host}:9094",}
admin_client = AdminClient(admin_client_config)

if args.submit:
    # Get producer
    conf = {'bootstrap.servers': os.environ["TARXIV_KAFKA_HOST"],
            'queue.buffering.max.messages': 1000,
            'queue.buffering.max.ms': 5000,
            'batch.num.messages': 16,
            'client.id': "cmdline"}
    producer = Producer(conf)

    # Read in json file
    with open(args.submit, "r") as f:
        json_data = json.load(f)

    # Either submit list or single dict
    if type(json_data) == list:
        for item in json_data:
            # Submit
            producer.produce(topic=args.topic, value=json.dumps(item).encode("utf-8"))
    # SINGLE
    else:
        # Submit
        producer.produce(topic=args.topic, value=json.dumps(json_data).encode("utf-8"))

    # FLush before leaving
    producer.flush()

elif args.list:
    metadata = admin_client.list_topics()
    # Iterate over the topic names (keys of the metadata.topics dictionary)
    print("TOPICS")
    for topic_name in metadata.topics:
        if "spark" in topic_name or True:
            print(f"- {topic_name}")

elif args.delete:
    print("DELETING")
    futures = admin_client.delete_topics([args.delete])
    for topic, future in futures.items():
        future.result()


elif args.create:
    print("CREATING")
    new = NewTopic(args.create, num_partitions=args.partitions, replication_factor=args.replication)
    futures = admin_client.create_topics([new])
    for topic, future in futures.items():
        future.result()

elif args.explain:
    conf = {'bootstrap.servers': f"{kafka_host}:9092",
            'group.id': 'testing1',
            'auto.offset.reset': 'smallest',
            'enable.auto.commit': True,
            'max.poll.interval.ms': 10000,
            'session.timeout.ms': 10000,
            'heartbeat.interval.ms': 3000,
            'enable.partition.eof': False}
    consumer = Consumer(conf)
    try:
        topic_name = args.explain
        # Get topic metadata to find all partitions
        metadata = admin_client.list_topics(topic_name, timeout=10)
        if topic_name not in metadata.topics:
            print(f"Error: Topic '{topic_name}' not found.")
            exit()

        topic_metadata = metadata.topics[topic_name]
        partitions = list(topic_metadata.partitions.keys())

        # Create TopicPartition list to query offsets
        topic_partitions = [_TopicPartition(topic_name, p) for p in partitions]

        print(f"--- Topic: {topic_name} ---")
        print(f"{'Partition':<10} | {'Low Offset':<12} | {'High Offset':<12} | {'Message Count':<15}")

        for tp in topic_partitions:
            # Get the low (earliest) and high (latest/log end) offsets
            try:
                # get_watermark_offsets gets the earliest and latest offset
                low_offset, high_offset = consumer.get_watermark_offsets(tp, timeout=5)
                message_count = max(0, high_offset - low_offset)
                print(f"{tp.partition:<10} | {low_offset:<12} | {high_offset:<12} | {message_count:<15}")
            except KafkaException as e:
                print(f"{tp.partition:<10} | Error getting offsets: {e}")

    except KafkaException as e:
        print(f"Error with AdminClient or general Kafka operation: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
    finally:
        consumer.close()